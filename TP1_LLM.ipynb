{
 "cells": [
  {
   "cell_type": "code",
   "id": "98397afc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T15:06:40.068380Z",
     "start_time": "2025-11-21T15:06:39.992575Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Lit le fichier .env et charge les variables\n",
    "\n",
    "# On r√©cup√®re la variable en utilisant son NOM\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\") \n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "ac3a6f1f",
   "metadata": {},
   "source": [
    "üëâ Exercice 3.1 ‚Äî En adaptant le script, affichez √©galement response.usage_metadata afin d'inspecter le nombre de tokens consomm√©s. Calculez ensuite un co√ªt approximatif en vous basant sur le tarif public (input_tokens et output_tokens)."
   ]
  },
  {
   "cell_type": "code",
   "id": "6cdb8cba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T15:06:45.341704Z",
     "start_time": "2025-11-21T15:06:40.089925Z"
    }
   },
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"ministral-3b-2410\",\n",
    "    temperature=0,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "message = HumanMessage(content=\"Quelle est la capitale de l'Albanie ?\")\n",
    "response = llm.invoke([message])\n",
    "\n",
    "print(response.content)\n",
    "print(response.usage_metadata)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capitale de l'Albanie est Tirana.\n",
      "{'input_tokens': 14, 'output_tokens': 12, 'total_tokens': 26}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "3c02defe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T15:06:46.597954Z",
     "start_time": "2025-11-21T15:06:46.585434Z"
    }
   },
   "source": [
    "# ---- PARAM√àTRES ----\n",
    "prix_input = 0.0005   # prix par 1k input tokens (exemple)\n",
    "prix_output = 0.0015  # prix par 1k output tokens (exemple)\n",
    "\n",
    "# ---- VALEURS ----\n",
    "input_tokens = 12000\n",
    "output_tokens = 8000\n",
    "\n",
    "# ---- CALCUL ----\n",
    "cout = (input_tokens/1000) * prix_input + (output_tokens/1000) * prix_output\n",
    "\n",
    "print(f\"Co√ªt total approximatif : {cout:.4f} $\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co√ªt total approximatif : 0.0180 $\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "e17a376d",
   "metadata": {},
   "source": [
    "üëâ Exercice 3.2 ‚Äî Modifiez la temp√©rature (temperature=0.7) et observez les diff√©rences de r√©ponses pour une requ√™te cr√©ative."
   ]
  },
  {
   "cell_type": "code",
   "id": "0d90661a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T15:06:46.987247Z",
     "start_time": "2025-11-21T15:06:46.632954Z"
    }
   },
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"ministral-3b-2410\",\n",
    "    temperature=0.7)\n",
    "\n",
    "message = HumanMessage(content=\"Quelle est la capitale de l'Albanie ?\")\n",
    "response = llm.invoke([message])\n",
    "\n",
    "print(response.content)\n",
    "print(response.usage_metadata)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capitale de l'Albanie est Tirana.\n",
      "{'input_tokens': 14, 'output_tokens': 12, 'total_tokens': 26}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "8a622a39",
   "metadata": {},
   "source": [
    "Le fait de changer la temp√©rature n'a rien chang√© √† la r√©ponse. M√™me si on augmente jusqu'a 1 "
   ]
  },
  {
   "cell_type": "code",
   "id": "d9cdc144",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T15:06:48.502581Z",
     "start_time": "2025-11-21T15:06:47.006757Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "model = ChatMistralAI(model=\"ministral-3b-2410\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Fais-moi une blague sur le sujet : {sujet}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "for sujet in [\"pompier\", \"police\"]:\n",
    "    print(chain.invoke({\"sujet\": sujet}))\n",
    "    print(\"-\" * 10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourquoi les pompiers ne jouent-ils jamais au poker ? Parce qu'ils ne veulent pas que les cartes soient en feu !\n",
      "----------\n",
      "Pourquoi les policiers ne jouent-ils jamais au poker ? Parce qu'ils ne veulent pas se faire \"carr√©\" !\n",
      "----------\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "fb025ad0",
   "metadata": {},
   "source": [
    "üëâ Exercice 4.1 ‚Äî Remplacez la liste de sujets par une liste saisie dynamiquement (prompt utilisateur ou lecture d'un fichier). Ajoutez un contr√¥le qui emp√™che l'envoi d'une cha√Æne vide."
   ]
  },
  {
   "cell_type": "code",
   "id": "9399d680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T15:10:23.938588Z",
     "start_time": "2025-11-21T15:10:02.973062Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "model = ChatMistralAI(model=\"ministral-3b-2410\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Fais-moi une blague sur le sujet : {sujet}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# ‚úÖ Saisie dynamique\n",
    "sujets_input = input(\"Entrez une liste de sujets s√©par√©s par des virgules : \")\n",
    "sujets = [s.strip() for s in sujets_input.split(\",\")]\n",
    "\n",
    "for sujet in sujets:\n",
    "    if not sujet:   \n",
    "        print(\"‚õî Sujet vide ignor√©.\")\n",
    "        continue\n",
    "\n",
    "    print(chain.invoke({\"sujet\": sujet}))\n",
    "    print(\"-\" * 10)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourquoi les Fran√ßais aiment-ils les blagues ?\n",
      "\n",
      "- Parce qu'elles sont toujours en \"blague\" !\n",
      "----------\n",
      "Pourquoi Macron ne joue pas au poker ? Parce qu'il a peur de perdre sa \"main\" !\n",
      "----------\n",
      "Pourquoi les √©tudiants universitaires aiment-ils les blagues sur les math√©matiques ?\n",
      "\n",
      "- Parce qu'elles sont toujours en \"derni√®re\" !\n",
      "----------\n",
      "Pourquoi le Pen est-il mauvais en maths ? Parce qu'il ne peut pas faire de \"d√©calage\" !\n",
      "----------\n",
      "Pourquoi ChatGPT est-il si populaire ? Parce qu'il est tr√®s \"chat\" !\n",
      "----------\n",
      "Pourquoi les jumeaux Gemini ne jouent-ils jamais au poker ? Parce qu'ils ne veulent pas se faire \"double\" !\n",
      "----------\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "e85ba42f",
   "metadata": {},
   "source": [
    "üëâ Exercice 4.2 ‚Äî Cr√©ez une variante de la cha√Æne produisant des r√©ponses au format JSON (par exemple avec les cl√©s setup et punchline). Pour cela, adaptez le prompt et utilisez StrOutputParser() ou un parser JSON."
   ]
  },
  {
   "cell_type": "code",
   "id": "11e5777a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T15:12:03.056922Z",
     "start_time": "2025-11-21T15:12:02.479635Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "model = ChatMistralAI(model=\"ministral-3b-2410\", temperature=0)\n",
    "\n",
    "prompt_json = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Cr√©e une blague sur le sujet : {sujet}\n",
    "    Renvoie le r√©sultat uniquement au format JSON :\n",
    "\n",
    "    {{\n",
    "        \"setup\": \"Texte d‚Äôintroduction\",\n",
    "        \"punchline\": \"Chute\"\n",
    "    }}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain_json = prompt_json | model | output_parser\n",
    "\n",
    "print(chain_json.invoke({\"sujet\": \"d√©veloppeur\"}))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"setup\": \"Pourquoi les d√©veloppeurs de caf√© sont-ils toujours en retard ?\",\n",
      "    \"punchline\": \"Parce qu'ils ne savent jamais o√π sont les fichiers de configuration !\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "eb9b7f76",
   "metadata": {},
   "source": [
    "üëâ Exercice 4.3 ‚Äî Ajoutez un param√®tre temperature pass√© dynamiquement via chain.invoke pour comparer l'impact sur plusieurs sujets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
